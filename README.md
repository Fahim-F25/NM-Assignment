# NM-Assignment
<br>
# ðŸ“˜ README Description

-----01-----

## Logistic Regression Optimization using Gradient Descent and Newtonâ€™s Method

This project demonstrates the implementation and comparison of two popular optimization techniques â€” **Gradient Descent** and **Newtonâ€™s Method** â€” for minimizing the cost function of a **Logistic Regression** model.

-----02-----
### ðŸ”¹ Key Features

* Generates a *synthetic binary classification dataset* using scikit-learn.
* Implements logistic regression cost function with:

  * *Sigmoid function*
  * *Cost calculation (Negative Log-Likelihood)*
  * *Gradient computation*
  * *Hessian computation* (for Newtonâ€™s method)
* Optimizes logistic regression parameters (Î¸) using:


  -----03-----

  1. *Gradient Descent* â€“ iterative updates with a learning rate.
  2. *Newtonâ€™s Method* â€“ updates using both gradient and Hessian (second-order derivative).
* Compares *convergence speed* of both methods by plotting cost vs iteration.


-----04-----
### ðŸ”¹ Results

* *Gradient Descent* takes many small steps to converge, depending on the learning rate.
* *Newtonâ€™s Method* converges much faster in fewer iterations, but requires matrix inversion, which is computationally expensive for high-dimensional data.
* A *comparison plot* shows the difference in convergence behavior.
  

-----05-----

### ðŸ”¹ Technologies Used

* *Python*
* *NumPy* â€“ for mathematical operations
* *Scikit-learn* â€“ for dataset generation
* *Matplotlib* â€“ for visualization

